{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3OD47lpoynZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
        "\n",
        "\n",
        "class FastText(Model):\n",
        "\n",
        "    def __init__(self,\n",
        "                 maxlen,\n",
        "                 max_features,\n",
        "                 embedding_dims,\n",
        "                 class_num=1,\n",
        "                 last_activation='sigmoid'):\n",
        "        super(FastText, self).__init__()\n",
        "        self.maxlen = maxlen\n",
        "        self.max_features = max_features\n",
        "        self.embedding_dims = embedding_dims\n",
        "        self.class_num = class_num\n",
        "        self.last_activation = last_activation\n",
        "        self.embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)\n",
        "        self.avg_pooling = GlobalAveragePooling1D()\n",
        "        self.classifier = Dense(self.class_num, activation=self.last_activation)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        if len(inputs.get_shape()) != 2:\n",
        "            raise ValueError('The rank of inputs of FastText must be 2, but now is %d' % len(inputs.get_shape()))\n",
        "        if inputs.get_shape()[1] != self.maxlen:\n",
        "            raise ValueError('The maxlen of inputs of FastText must be %d, but now is %d' % (self.maxlen, inputs.get_shape()[1]))\n",
        "        embedding = self.embedding(inputs)\n",
        "        x = self.avg_pooling(embedding)\n",
        "        output = self.classifier(x)\n",
        "        return output\n",
        "\"\"\"\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "scores = []\n",
        "cv = KFold(n_splits=5, random_state=42, shuffle=True)\n",
        "t=time()\n",
        "for train_index, test_index in cv.split(X):\n",
        "    print(\"Train Index: \", train_index, \"\\n\")\n",
        "    print(\"Test Index: \", test_index)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
        "    X_train = tokenizer.texts_to_sequences(X_train)\n",
        "    X_test = tokenizer.texts_to_sequences(X_test)\n",
        "    max_features = len(tokenizer.word_index) + 1\n",
        "    X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "    X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "    maxlen = 50\n",
        "    batch_size = 64\n",
        "    embedding_dims = 50\n",
        "    input_dim = X_train.shape[1]  # Number of features\n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TAtB-ZD_2UL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Embedding, Dense, Concatenate, Conv1D, Bidirectional, LSTM, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
        "\n",
        "class RCNNVariant(Model):\n",
        "    \"\"\"Variant of RCNN.\n",
        "        Base on structure of RCNN, we do some improvement:\n",
        "        1. Ignore the shift for left/right context.\n",
        "        2. Use Bidirectional LSTM/GRU to encode context.\n",
        "        3. Use Multi-CNN to represent the semantic vectors.\n",
        "        4. Use ReLU instead of Tanh.\n",
        "        5. Use both AveragePooling and MaxPooling.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 maxlen,\n",
        "                 max_features,\n",
        "                 embedding_dims,\n",
        "                 kernel_sizes=[1, 2, 3, 4, 5],\n",
        "                 class_num=1,\n",
        "                 last_activation='sigmoid'):\n",
        "        super(RCNNVariant, self).__init__()\n",
        "        self.maxlen = maxlen\n",
        "        self.max_features = max_features\n",
        "        self.embedding_dims = embedding_dims\n",
        "        self.kernel_sizes = kernel_sizes\n",
        "        self.class_num = class_num\n",
        "        self.last_activation = last_activation\n",
        "        self.embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)\n",
        "        self.bi_rnn = Bidirectional(LSTM(128, return_sequences=True))\n",
        "        self.concatenate = Concatenate()\n",
        "        self.convs = []\n",
        "        for kernel_size in self.kernel_sizes:\n",
        "            conv = Conv1D(128, kernel_size, activation='relu')\n",
        "            self.convs.append(conv)\n",
        "        self.avg_pooling = GlobalAveragePooling1D()\n",
        "        self.max_pooling = GlobalMaxPooling1D()\n",
        "        self.classifier = Dense(self.class_num, activation=self.last_activation)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        if len(inputs.get_shape()) != 2:\n",
        "            raise ValueError('The rank of inputs of TextRNN must be 2, but now is %d' % len(inputs.get_shape()))\n",
        "        if inputs.get_shape()[1] != self.maxlen:\n",
        "            raise ValueError('The maxlen of inputs of TextRNN must be %d, but now is %d' % (self.maxlen, inputs.get_shape()[1]))\n",
        "        embedding = self.embedding(inputs)\n",
        "        x_context = self.bi_rnn(embedding)\n",
        "        x = self.concatenate([embedding, x_context])\n",
        "        convs = []\n",
        "        for i in range(len(self.kernel_sizes)):\n",
        "            conv = self.convs[i](x)\n",
        "            convs.append(conv)\n",
        "        poolings = [self.avg_pooling(conv) for conv in convs] + [self.max_pooling(conv) for conv in convs]\n",
        "        x = self.concatenate(poolings)\n",
        "        output = self.classifier(x)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ28Zslh_8F5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM, Bidirectional,SimpleRNN\n",
        "class TextBiRNN(Model):\n",
        "    def __init__(self,\n",
        "                 maxlen,\n",
        "                 max_features,\n",
        "                 embedding_dims,\n",
        "                 class_num=1,\n",
        "                 last_activation='sigmoid'):\n",
        "        super(TextBiRNN, self).__init__()\n",
        "        self.maxlen = maxlen\n",
        "        self.max_features = max_features\n",
        "        self.embedding_dims = embedding_dims\n",
        "        self.class_num = class_num\n",
        "        self.last_activation = last_activation\n",
        "        self.embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)\n",
        "        self.bi_rnn = Bidirectional(SimpleRNN(128))  # LSTM or GRU\n",
        "        self.classifier = Dense(self.class_num, activation=self.last_activation)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        if len(inputs.get_shape()) != 2:\n",
        "            raise ValueError('The rank of inputs of TextBiRNN must be 2, but now is %d' % len(inputs.get_shape()))\n",
        "        if inputs.get_shape()[1] != self.maxlen:\n",
        "            raise ValueError('The maxlen of inputs of TextBiRNN must be %d, but now is %d' % (self.maxlen, inputs.get_shape()[1]))\n",
        "        embedding = self.embedding(inputs)\n",
        "        x = self.bi_rnn(embedding)\n",
        "        output = self.classifier(x)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8TuKOZuAddE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Embedding, Dense, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout\n",
        "class TextCNN(Model):\n",
        "\n",
        "    def __init__(self,\n",
        "                 maxlen,\n",
        "                 max_features,\n",
        "                 embedding_dims,\n",
        "                 kernel_sizes=[3, 4, 5],\n",
        "                 class_num=1,\n",
        "                 last_activation='sigmoid'):\n",
        "        super(TextCNN, self).__init__()\n",
        "        self.maxlen = maxlen\n",
        "        self.max_features = max_features\n",
        "        self.embedding_dims = embedding_dims\n",
        "        self.kernel_sizes = kernel_sizes\n",
        "        self.class_num = class_num\n",
        "        self.last_activation = last_activation\n",
        "        self.embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)\n",
        "        self.convs = []\n",
        "        self.max_poolings = []\n",
        "        for kernel_size in self.kernel_sizes:\n",
        "            self.convs.append(Conv1D(128, kernel_size, activation='relu'))\n",
        "            self.max_poolings.append(GlobalMaxPooling1D())\n",
        "        self.classifier = Dense(self.class_num, activation=self.last_activation)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        if len(inputs.get_shape()) != 2:\n",
        "            raise ValueError('The rank of inputs of TextCNN must be 2, but now is %d' % len(inputs.get_shape()))\n",
        "        if inputs.get_shape()[1] != self.maxlen:\n",
        "            raise ValueError('The maxlen of inputs of TextCNN must be %d, but now is %d' % (self.maxlen, inputs.get_shape()[1]))\n",
        "        # Embedding part can try multichannel as same as origin paper\n",
        "        embedding = self.embedding(inputs)\n",
        "        convs = []\n",
        "        for i in range(len(self.kernel_sizes)):\n",
        "            c = self.convs[i](embedding)\n",
        "            c = self.max_poolings[i](c)\n",
        "            convs.append(c)\n",
        "        x = Concatenate()(convs)\n",
        "        output = self.classifier(x)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxPfPUwtAlRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM,GRU, SimpleRNN\n",
        "class TextRNN(Model):\n",
        "    def __init__(self,\n",
        "                 maxlen,\n",
        "                 max_features,\n",
        "                 embedding_dims,\n",
        "                 class_num=1,\n",
        "                 last_activation='sigmoid'):\n",
        "        super(TextRNN, self).__init__()\n",
        "        self.maxlen = maxlen\n",
        "        self.max_features = max_features\n",
        "        self.embedding_dims = embedding_dims\n",
        "        self.class_num = class_num\n",
        "        self.last_activation = last_activation\n",
        "        self.embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)\n",
        "        self.rnn = SimpleRNN(128)  # LSTM or GRU\n",
        "        self.classifier = Dense(self.class_num, activation=self.last_activation)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        if len(inputs.get_shape()) != 2:\n",
        "            raise ValueError('The rank of inputs of TextRNN must be 2, but now is %d' % len(inputs.get_shape()))\n",
        "        if inputs.get_shape()[1] != self.maxlen:\n",
        "            raise ValueError('The maxlen of inputs of TextRNN must be %d, but now is %d' % (self.maxlen, inputs.get_shape()[1]))\n",
        "        embedding = self.embedding(inputs)\n",
        "        x = self.rnn(embedding)\n",
        "        output = self.classifier(x)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeDEzw6T4au3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmV1FvLPuuni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from time import time\n",
        "\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "d1 = pd.read_csv(\"/content/drive/My Drive/Name_Gender.csv\")\n",
        "y = d1['gender']\n",
        "X = d1['first_name']\n",
        "X = X.astype(str)\n",
        "\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X)\n",
        "\n",
        "maxlen = 50\n",
        "batch_size = 64\n",
        "embedding_dims = 50\n",
        "epochs = 10\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "max_features = len(tokenizer.word_index) + 1\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "input_dim = X_train.shape[1]  # Number of features\n",
        "\"\"\"\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(input_dim=vocab_size, \n",
        "                           output_dim=embedding_dims, \n",
        "                           input_length=maxlen))\n",
        "#model.add(layers.Flatten())\n",
        "model.add(layers.GlobalAveragePooling1D())\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\"\"\"\n",
        "model = TextBiRNN(maxlen, max_features, embedding_dims)\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "#model.summary()\n",
        "\n",
        "t=time()\n",
        "history = model.fit(X_train, y_train,epochs=20,verbose=False,validation_data=(X_test, y_test),batch_size=64)\n",
        "print(time()-t)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "print(model.count_params())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}